# Cell 1

!pip install mediapipe opencv-python numpy

# Cell 2: Optimized Helper Functions
import cv2
import mediapipe as mp
import numpy as np
import math


mp_drawing = mp.solutions.drawing_utils
mp_pose = mp.solutions.pose

# Pre-compute constants for speed
SQRT = math.sqrt
ACOS = math.acos
DEGREES = math.degrees

def compute_distance(point_a, point_b):
    """Fast distance calculation"""
    dx = point_a[0] - point_b[0]
    dy = point_a[1] - point_b[1]
    return SQRT(dx * dx + dy * dy)

def compute_angle(point_a, point_b, point_c):
    """Fast angle calculation with safety checks"""
    ax, ay = point_a
    bx, by = point_b
    cx, cy = point_c
    
    # Vector BA
    ba_x = ax - bx
    ba_y = ay - by
    
    # Vector BC
    bc_x = cx - bx
    bc_y = cy - by
    
    # Dot product
    dot_product = ba_x * bc_x + ba_y * bc_y
    
    # Magnitudes
    mag_ba = SQRT(ba_x * ba_x + ba_y * ba_y)
    mag_bc = SQRT(bc_x * bc_x + bc_y * bc_y)
    
    # Early return for invalid vectors
    if mag_ba < 1e-6 or mag_bc < 1e-6:
        return 0.0
    
    # Clamp and calculate
    cos_angle = max(-1.0, min(1.0, dot_product / (mag_ba * mag_bc)))
    return DEGREES(ACOS(cos_angle))

def compute_angle_from_vertical(point_a, point_b):
    """Fast vertical angle calculation"""
    # Vector from B to A
    vec_x = point_a[0] - point_b[0]
    vec_y = point_a[1] - point_b[1]
    
    # Dot product with vertical (0, -1)
    dot_product = -vec_y  # Simplified since vert_x = 0
    
    # Magnitude
    mag_vec = SQRT(vec_x * vec_x + vec_y * vec_y)
    
    if mag_vec < 1e-6:
        return 0.0
    
    cos_angle = max(-1.0, min(1.0, dot_product / mag_vec))
    return DEGREES(ACOS(cos_angle))

# Clinical thresholds (based on rehabilitation research)
THRESHOLDS = {
    'elbow': {
        'critical_max': 179,   # Above 179Â° = locked
        'warning_max': 170,    # Above 170Â° = too straight
        'optimal_max': 165,    # Optimal upper bound
        'optimal_min': 150,    # Optimal lower bound (30Â° flexion)
        'warning_min': 140,    # Below 140Â° = too bent
        'critical_min': 120    # Below 120Â° = way too bent
    },
    'trunk_lean': {
        'optimal_max': 15,     # Up to 15Â° is normal
        'warning_max': 20,     # 15-20Â° = caution
        'critical_max': 25     # Above 20Â° = dangerous
    },
    'knee_stance': {
        'warning_min': 160,    # Below 160Â° = too bent
        'optimal_min': 165,    # Optimal: 165-175Â° (slight flexion)
        'optimal_max': 175,
        'warning_max': 178     # Above 178Â° = locked knee
    },
    'shoulder_asym': {
        'optimal_max': 3,      # Up to 3% is normal asymmetry
        'warning_max': 5,      # 3-5% = caution
        'critical_max': 8      # Above 5% = problem
    },
    'step_length': {
        'critical_min': 15,    # Below 15 cm = shuffling
        'warning_min': 20,
        'optimal_min': 25,     # Optimal: 25-50 cm
        'optimal_max': 50,
        'warning_max': 60,
        'critical_max': 75     # Above 60 cm = overstriding
    },
    'base_width': {
        'critical_min': 8,     # Below 8 cm = very narrow
        'warning_min': 10,
        'optimal_min': 15,     # Optimal: 15-30 cm (roughly hip width)
        'optimal_max': 30,
        'warning_max': 40,
        'critical_max': 50     # Above 40 cm = very wide
    }
}

def evaluate_trunk_lean(angle):
    """Evaluate trunk lean angle."""
    t = THRESHOLDS['trunk_lean']
    
    if angle > t['critical_max']:
        return {
            'status': 'critical',
            'message': "Leaning too far forward - stand upright",
            'color': (0, 0, 255)
        }
    elif angle > t['warning_max']:
        return {
            'status': 'warning',
            'message': "Excessive forward lean",
            'color': (0, 165, 255)
        }
    elif angle <= t['optimal_max']:
        return {
            'status': 'good',
            'message': None,
            'color': (0, 255, 0)
        }
    else:
        return {
            'status': 'caution',
            'message': None,
            'color': (0, 255, 255)
        }
    
def evaluate_knee(angle, side='right'):
    """Evaluate knee angle (for standing/stance)."""
    t = THRESHOLDS['knee_stance']
    
    if angle < t['warning_min']:
        return {
            'status': 'warning',
            'message': f"{side.upper()} knee too bent",
            'color': (0, 165, 255)
        }
    elif angle > t['warning_max']:
        return {
            'status': 'warning',
            'message': f"{side.upper()} knee locked - allow slight bend",
            'color': (0, 165, 255)
        }
    elif t['optimal_min'] <= angle <= t['optimal_max']:
        return {
            'status': 'good',
            'message': None,
            'color': (0, 255, 0)
        }
    else:
        return {
            'status': 'caution',
            'message': None,
            'color': (0, 255, 255)
        }

def evaluate_shoulder_asym(percent):
    """Evaluate shoulder asymmetry (as % of leg length)."""
    t = THRESHOLDS['shoulder_asym']
    
    if percent > t['critical_max']:
        return {
            'status': 'critical',
            'message': "Shoulders very uneven - check crutch height",
            'color': (0, 0, 255)
        }
    elif percent > t['warning_max']:
        return {
            'status': 'warning',
            'message': "Shoulders uneven",
            'color': (0, 165, 255)
        }
    elif percent <= t['optimal_max']:
        return {
            'status': 'good',
            'message': None,
            'color': (0, 255, 0)
        }
    else:
        return {
            'status': 'caution',
            'message': None,
            'color': (0, 255, 255)
        }

def evaluate_step_length(length_cm):
    """Evaluate step length in cm."""
    t = THRESHOLDS['step_length']
    
    if length_cm < t['critical_min']:
        return {
            'status': 'critical',
            'message': "Steps very short - try longer strides",
            'color': (0, 0, 255)
        }
    elif length_cm < t['warning_min']:
        return {
            'status': 'warning',
            'message': "Steps too short",
            'color': (0, 165, 255)
        }
    elif length_cm > t['critical_max']:
        return {
            'status': 'critical',
            'message': "Steps too long - reduce stride",
            'color': (0, 0, 255)
        }
    elif length_cm > t['warning_max']:
        return {
            'status': 'warning',
            'message': "Steps too long",
            'color': (0, 165, 255)
        }
    elif t['optimal_min'] <= length_cm <= t['optimal_max']:
        return {
            'status': 'good',
            'message': None,
            'color': (0, 255, 0)
        }
    else:
        return {
            'status': 'caution',
            'message': None,
            'color': (0, 255, 255)
        }

def evaluate_base_width(width_cm):
    """Evaluate base of support width in cm."""
    t = THRESHOLDS['base_width']
    
    if width_cm < t['critical_min']:
        return {
            'status': 'critical',
            'message': "Stance very narrow - widen for stability",
            'color': (0, 0, 255)
        }
    elif width_cm < t['warning_min']:
        return {
            'status': 'warning',
            'message': "Stance too narrow",
            'color': (0, 165, 255)
        }
    elif width_cm > t['critical_max']:
        return {
            'status': 'critical',
            'message': "Stance very wide - bring feet closer",
            'color': (0, 0, 255)
        }
    elif width_cm > t['warning_max']:
        return {
            'status': 'warning',
            'message': "Stance too wide",
            'color': (0, 165, 255)
        }
    elif t['optimal_min'] <= width_cm <= t['optimal_max']:
        return {
            'status': 'good',
            'message': None,
            'color': (0, 255, 0)
        }
    else:
        return {
            'status': 'caution',
            'message': None,
            'color': (0, 255, 255)
        }

print("âœ“ Helper functions loaded")

# Cell 4: User Input & Calibration Setup

print("\n" + "="*50)
print("CRUTCH GAIT ANALYSIS - SETUP")
print("="*50)

# Get user height
print("\n=== STEP 1: Enter Your Height ===")
height_unit = input("Use (f)eet or (c)m? ").strip().lower()

if height_unit in ['f', 'feet', 'ft']:
    feet = float(input("Feet: "))
    inches = float(input("Inches: "))
    user_height_cm = (feet * 30.48) + (inches * 2.54)
else:
    user_height_cm = float(input("Height (cm): "))

# Calculate expected body segment lengths using anthropometric ratios
expected_leg_length_cm = user_height_cm * 0.53  # Legs are ~53% of height
expected_arm_length_cm = user_height_cm * 0.28  # Arms are ~28% of height

print(f"\nâœ“ Your height: {user_height_cm:.1f} cm")
print(f"âœ“ Expected leg length: {expected_leg_length_cm:.1f} cm")

# Initialize calibration variables
calibration_complete = False
calibration_frames = []
pixels_per_cm = None

print("\n=== STEP 2: Calibration Instructions ===")
print("1. Stand straight, facing the camera")
print("2. Keep both legs straight, arms at sides")
print("3. Wait for automatic calibration")
print("4. Stay at same distance while walking")
print("\nCalibration will start automatically...")
print("="*50)

# Cell 3: Optimized Calibration UI Function

def draw_calibration_overlay(image, phase, countdown=None, visibility_check=None):
    """
    Optimized calibration overlays - minimal copying, direct drawing
    """
    height, width = image.shape[:2]
    
    if phase == 'positioning':
        # Darken in-place (fast)
        image[:] = (image * 0.5).astype(np.uint8)
        
        # Header box
        header_y = 40
        header_h = 80
        cv2.rectangle(image, (width//2 - 300, header_y), 
                     (width//2 + 300, header_y + header_h), 
                     (50, 50, 50), -1)
        cv2.rectangle(image, (width//2 - 300, header_y), 
                     (width//2 + 300, header_y + header_h), 
                     (255, 255, 255), 2)
        
        cv2.putText(image, "Position yourself 5-7 feet away", 
                   (width//2 - 280, header_y + 55), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)
        
        # Visibility checklist (compact)
        if visibility_check:
            y_start = 180
            checks = [
                ("Head", visibility_check.get('head', False)),
                ("Arms", visibility_check.get('arms', False)),
                ("Legs", visibility_check.get('legs', False)),
                ("Distance", visibility_check.get('distance', False))
            ]
            
            for i, (label, status) in enumerate(checks):
                color = (0, 255, 0) if status else (0, 100, 255)
                symbol = "âœ“" if status else "â—‹"
                cv2.putText(image, f"{symbol} {label}", 
                           (width//2 - 100, y_start + i*35), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        return image
    
    elif phase == 'ready':
        # Light darken
        image[:] = (image * 0.7).astype(np.uint8)
        
        # Compact green box
        box_w, box_h = 350, 80
        cv2.rectangle(image, (width//2 - box_w//2, height//2 - box_h//2), 
                     (width//2 + box_w//2, height//2 + box_h//2), 
                     (30, 80, 30), -1)
        cv2.rectangle(image, (width//2 - box_w//2, height//2 - box_h//2), 
                     (width//2 + box_w//2, height//2 + box_h//2), 
                     (0, 255, 0), 3)
        
        cv2.putText(image, "Prepare to move...", 
                   (width//2 - 140, height//2 + 15), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)
        
        return image
    
    elif phase == 'countdown':
        # Medium darken
        image[:] = (image * 0.6).astype(np.uint8)
        
        # Giant countdown number
        text = str(countdown)
        font_scale = 6
        thickness = 12
        text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)[0]
        text_x = (width - text_size[0]) // 2
        text_y = (height + text_size[1]) // 2
        
        # Single draw (no outline for performance)
        cv2.putText(image, text, (text_x, text_y), 
                   cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 255, 255), thickness)
        
        return image
    
    elif phase == 'calibrating':
        # Minimal overlay - just status bar
        bar_w = 450
        bar_h = 70
        cv2.rectangle(image, (width//2 - bar_w//2, 20), 
                     (width//2 + bar_w//2, 20 + bar_h), 
                     (40, 40, 40), -1)
        cv2.rectangle(image, (width//2 - bar_w//2, 20), 
                     (width//2 + bar_w//2, 20 + bar_h), 
                     (0, 255, 255), 2)
        
        cv2.putText(image, "CALIBRATING... Stand still!", 
                   (width//2 - 200, 60), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
        
        return image
    
    return image

print("âœ“ Optimized Calibration UI loaded")

# Cell 5: Main Processing Loop

import signal
import sys

# ========================================
# EXIT HANDLER SETUP
# ========================================

def cleanup_and_exit(cap):
    """Properly release resources and exit"""
    print("\nðŸ›‘ Shutting down gracefully...")
    cap.release()
    cv2.destroyAllWindows()
    print("âœ“ Camera released")
    print("âœ“ Windows closed")
    sys.exit(0)

def signal_handler(sig, frame):
    """Handle Ctrl+C interrupt"""
    print("\nâš ï¸  Interrupt received!")
    cleanup_and_exit(cap)

# ========================================
# CALIBRATION STATE MACHINE SETUP
# ========================================

PHASE_POSITIONING = 'positioning'
PHASE_READY = 'ready'
PHASE_COUNTDOWN = 'countdown'
PHASE_CALIBRATING = 'calibrating'
PHASE_COMPLETE = 'complete'

# Initialize tracking variables
calibration_phase = PHASE_POSITIONING
countdown_start_time = None
countdown_value = 3
positioning_frames_good = 0

# Performance optimization: reduce resolution
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
cap.set(cv2.CAP_PROP_FPS, 30)

# Register Ctrl+C handler AFTER cap is created
signal.signal(signal.SIGINT, signal_handler)

# Create named window that can be closed with X
cv2.namedWindow("Mediapipe Feed", cv2.WINDOW_NORMAL)

# Pre-define landmark indices for faster access
NOSE = mp_pose.PoseLandmark.NOSE.value
LEFT_WRIST = mp_pose.PoseLandmark.LEFT_WRIST.value
RIGHT_WRIST = mp_pose.PoseLandmark.RIGHT_WRIST.value
LEFT_ANKLE = mp_pose.PoseLandmark.LEFT_ANKLE.value
RIGHT_ANKLE = mp_pose.PoseLandmark.RIGHT_ANKLE.value
LEFT_HIP = mp_pose.PoseLandmark.LEFT_HIP.value
RIGHT_HIP = mp_pose.PoseLandmark.RIGHT_HIP.value
LEFT_SHOULDER = mp_pose.PoseLandmark.LEFT_SHOULDER.value
RIGHT_SHOULDER = mp_pose.PoseLandmark.RIGHT_SHOULDER.value
LEFT_ELBOW = mp_pose.PoseLandmark.LEFT_ELBOW.value
RIGHT_ELBOW = mp_pose.PoseLandmark.RIGHT_ELBOW.value
LEFT_KNEE = mp_pose.PoseLandmark.LEFT_KNEE.value
RIGHT_KNEE = mp_pose.PoseLandmark.RIGHT_KNEE.value
LEFT_FOOT_INDEX = mp_pose.PoseLandmark.LEFT_FOOT_INDEX.value
RIGHT_FOOT_INDEX = mp_pose.PoseLandmark.RIGHT_FOOT_INDEX.value

with mp_pose.Pose(
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5,
        model_complexity=0) as pose:  # Use fastest model (0 = lite)
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        # Flip frame for mirror effect
        frame = cv2.flip(frame, 1)
        
        # Convert BGR â†’ RGB
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        image.flags.writeable = False
        results = pose.process(image)
        
        # Convert back RGB â†’ BGR
        image.flags.writeable = True
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        
        # Get image dimensions once
        image_height, image_width = image.shape[:2]
        
        # ========================================
        # CALIBRATION FLOW
        # ========================================
        
        if not calibration_complete:
            
            if results.pose_landmarks:
                landmarks = results.pose_landmarks.landmark
                
                # Fast visibility checks
                head_visible = landmarks[NOSE].visibility > 0.5
                arms_visible = (landmarks[LEFT_WRIST].visibility > 0.5 and 
                               landmarks[RIGHT_WRIST].visibility > 0.5)
                legs_visible = (landmarks[LEFT_ANKLE].visibility > 0.5 and 
                               landmarks[RIGHT_ANKLE].visibility > 0.5)
                
                # Distance check
                hip_y = (landmarks[LEFT_HIP].y + landmarks[RIGHT_HIP].y) * 0.5
                ankle_y = (landmarks[LEFT_ANKLE].y + landmarks[RIGHT_ANKLE].y) * 0.5
                body_height_pixels = abs((ankle_y - hip_y) * image_height)
                good_distance = 200 < body_height_pixels < 500
                
                all_checks_pass = head_visible and arms_visible and legs_visible and good_distance
                
                visibility_status = {
                    'head': head_visible,
                    'arms': arms_visible,
                    'legs': legs_visible,
                    'distance': good_distance
                }
                
                # PHASE 1: POSITIONING
                if calibration_phase == PHASE_POSITIONING:
                    image = draw_calibration_overlay(image, PHASE_POSITIONING, 
                                                     visibility_check=visibility_status)
                    
                    if all_checks_pass:
                        positioning_frames_good += 1
                        if positioning_frames_good >= 30:
                            calibration_phase = PHASE_READY
                            countdown_start_time = cv2.getTickCount()
                    else:
                        positioning_frames_good = 0
                
                # PHASE 2: READY
                elif calibration_phase == PHASE_READY:
                    image = draw_calibration_overlay(image, PHASE_READY)
                    
                    elapsed = (cv2.getTickCount() - countdown_start_time) / cv2.getTickFrequency()
                    if elapsed >= 1.5:
                        calibration_phase = PHASE_COUNTDOWN
                        countdown_start_time = cv2.getTickCount()
                        countdown_value = 3
                
                # PHASE 3: COUNTDOWN
                elif calibration_phase == PHASE_COUNTDOWN:
                    elapsed = (cv2.getTickCount() - countdown_start_time) / cv2.getTickFrequency()
                    countdown_value = 3 - int(elapsed)
                    
                    if countdown_value > 0:
                        image = draw_calibration_overlay(image, PHASE_COUNTDOWN, 
                                                         countdown=countdown_value)
                    else:
                        calibration_phase = PHASE_CALIBRATING
                
                # PHASE 4: CALIBRATING
                elif calibration_phase == PHASE_CALIBRATING:
                    mp_drawing.draw_landmarks(image, results.pose_landmarks, 
                                             mp_pose.POSE_CONNECTIONS)
                    
                    # Only extract 4 landmarks needed for calibration
                    lh = landmarks[LEFT_HIP]
                    rh = landmarks[RIGHT_HIP]
                    la = landmarks[LEFT_ANKLE]
                    ra = landmarks[RIGHT_ANKLE]
                    
                    lx_hip, ly_hip = lh.x * image_width, lh.y * image_height
                    rx_hip, ry_hip = rh.x * image_width, rh.y * image_height
                    lx_ankle, ly_ankle = la.x * image_width, la.y * image_height
                    rx_ankle, ry_ankle = ra.x * image_width, ra.y * image_height
                    
                    # Measure leg length
                    left_leg_pixels = compute_distance((lx_hip, ly_hip), (lx_ankle, ly_ankle))
                    right_leg_pixels = compute_distance((rx_hip, ry_hip), (rx_ankle, ry_ankle))
                    avg_leg_pixels = (left_leg_pixels + right_leg_pixels) * 0.5
                    
                    calibration_frames.append(avg_leg_pixels)
                    
                    # Show calibrating overlay
                    image = draw_calibration_overlay(image, PHASE_CALIBRATING)
                    cv2.putText(image, f"Frames: {len(calibration_frames)}/30", 
                        (image_width//2 - 80, 90), cv2.FONT_HERSHEY_SIMPLEX, 
                        0.7, (0, 255, 255), 2)
                    
                    if len(calibration_frames) >= 30:
                        avg_calibration_pixels = np.mean(calibration_frames)
                        pixels_per_cm = avg_calibration_pixels / expected_leg_length_cm
                        calibration_complete = True
                        calibration_phase = PHASE_COMPLETE
                        print(f"\nâœ“ Calibration complete!")
                        print(f"  Conversion factor: {pixels_per_cm:.2f} pixels/cm")
            
            else:
                # No pose detected
                if calibration_phase == PHASE_POSITIONING:
                    image = draw_calibration_overlay(image, PHASE_POSITIONING, 
                                                     visibility_check={
                                                         'head': False,
                                                         'arms': False,
                                                         'legs': False,
                                                         'distance': False
                                                     })
            
            # Show and continue
            cv2.imshow("Mediapipe Feed", image)
            
            # Check if window was closed
            if cv2.getWindowProperty("Mediapipe Feed", cv2.WND_PROP_VISIBLE) < 1:
                cleanup_and_exit(cap)
            
            # Check for exit keys
            key = cv2.waitKey(10) & 0xFF
            if key == ord('q') or key == 27:
                cleanup_and_exit(cap)
            
            continue
        
        # ========================================
        # MAIN ANALYSIS (After calibration)
        # ========================================
        
        if results.pose_landmarks:
            mp_drawing.draw_landmarks(image, results.pose_landmarks, 
                                     mp_pose.POSE_CONNECTIONS)
            
            # Fast landmark extraction using direct indexing
            landmarks = results.pose_landmarks.landmark
            
            # Convert to pixels
            lx_shoulder = landmarks[LEFT_SHOULDER].x * image_width
            ly_shoulder = landmarks[LEFT_SHOULDER].y * image_height
            rx_shoulder = landmarks[RIGHT_SHOULDER].x * image_width
            ry_shoulder = landmarks[RIGHT_SHOULDER].y * image_height
            
            lx_elbow = landmarks[LEFT_ELBOW].x * image_width
            ly_elbow = landmarks[LEFT_ELBOW].y * image_height
            rx_elbow = landmarks[RIGHT_ELBOW].x * image_width
            ry_elbow = landmarks[RIGHT_ELBOW].y * image_height
            
            lx_wrist = landmarks[LEFT_WRIST].x * image_width
            ly_wrist = landmarks[LEFT_WRIST].y * image_height
            rx_wrist = landmarks[RIGHT_WRIST].x * image_width
            ry_wrist = landmarks[RIGHT_WRIST].y * image_height
            
            lx_hip = landmarks[LEFT_HIP].x * image_width
            ly_hip = landmarks[LEFT_HIP].y * image_height
            rx_hip = landmarks[RIGHT_HIP].x * image_width
            ry_hip = landmarks[RIGHT_HIP].y * image_height
            
            lx_knee = landmarks[LEFT_KNEE].x * image_width
            ly_knee = landmarks[LEFT_KNEE].y * image_height
            rx_knee = landmarks[RIGHT_KNEE].x * image_width
            ry_knee = landmarks[RIGHT_KNEE].y * image_height
            
            lx_ankle = landmarks[LEFT_ANKLE].x * image_width
            ly_ankle = landmarks[LEFT_ANKLE].y * image_height
            rx_ankle = landmarks[RIGHT_ANKLE].x * image_width
            ry_ankle = landmarks[RIGHT_ANKLE].y * image_height
            
            lx_foot = landmarks[LEFT_FOOT_INDEX].x * image_width
            ly_foot = landmarks[LEFT_FOOT_INDEX].y * image_height
            rx_foot = landmarks[RIGHT_FOOT_INDEX].x * image_width
            ry_foot = landmarks[RIGHT_FOOT_INDEX].y * image_height
            
            # ========================================
            # Compute distances
            # ========================================
            
            # Leg lengths
            left_leg_length_px = compute_distance((lx_hip, ly_hip), (lx_ankle, ly_ankle))
            right_leg_length_px = compute_distance((rx_hip, ry_hip), (rx_ankle, ry_ankle))
            
            # Convert to cm (multiply is faster than divide)
            inv_pixels_per_cm = 1.0 / pixels_per_cm
            left_leg_length = left_leg_length_px * inv_pixels_per_cm
            right_leg_length = right_leg_length_px * inv_pixels_per_cm
            avg_leg_length = (left_leg_length + right_leg_length) * 0.5
            
            # Hip width
            hip_width_px = compute_distance((lx_hip, ly_hip), (rx_hip, ry_hip))
            hip_width = hip_width_px * inv_pixels_per_cm
            
            # Step length
            step_length_px = abs(rx_ankle - lx_ankle)
            step_length = step_length_px * inv_pixels_per_cm
            
            # Base of support
            base_of_support_px = compute_distance((lx_foot, ly_foot), (rx_foot, ry_foot))
            base_of_support = base_of_support_px * inv_pixels_per_cm
            
            # ========================================
            # Compute angles
            # ========================================
            
            right_elbow_angle = compute_angle(
                (rx_shoulder, ry_shoulder),
                (rx_elbow, ry_elbow),
                (rx_wrist, ry_wrist)
            )
            
            left_elbow_angle = compute_angle(
                (lx_shoulder, ly_shoulder),
                (lx_elbow, ly_elbow),
                (lx_wrist, ly_wrist)
            )
            
            right_knee_angle = compute_angle(
                (rx_hip, ry_hip),
                (rx_knee, ry_knee),
                (rx_ankle, ry_ankle)
            )
            
            left_knee_angle = compute_angle(
                (lx_hip, ly_hip),
                (lx_knee, ly_knee),
                (lx_ankle, ly_ankle)
            )
            
            # Midpoints
            mid_hip_x = (lx_hip + rx_hip) * 0.5
            mid_hip_y = (ly_hip + ry_hip) * 0.5
            mid_shoulder_x = (lx_shoulder + rx_shoulder) * 0.5
            mid_shoulder_y = (ly_shoulder + ry_shoulder) * 0.5
            
            trunk_lean_angle = compute_angle_from_vertical(
                (mid_shoulder_x, mid_shoulder_y),
                (mid_hip_x, mid_hip_y)
            )
            
            # Asymmetry
            shoulder_height_diff = abs(ly_shoulder - ry_shoulder)
            shoulder_elevation_percent = (shoulder_height_diff / avg_leg_length) * 100
            
            # ========================================
            # EVALUATE MEASUREMENTS
            # ========================================

            evaluations = {
                'elbow_r': evaluate_elbow(right_elbow_angle, 'right'),
                'elbow_l': evaluate_elbow(left_elbow_angle, 'left'),
                'knee_r': evaluate_knee(right_knee_angle, 'right'),
                'knee_l': evaluate_knee(left_knee_angle, 'left'),
                'trunk': evaluate_trunk_lean(trunk_lean_angle),
                'shoulder': evaluate_shoulder_asym(shoulder_elevation_percent),
                'step': evaluate_step_length(step_length),
                'base': evaluate_base_width(base_of_support)
            }

            # Collect feedback messages (only issues)
            feedback_messages = []
            for key, eval_result in evaluations.items():
                if eval_result['message'] is not None:
                    feedback_messages.append(eval_result['message'])

            # Sort by severity (critical first, then warning, then caution)
            severity_order = {'critical': 0, 'warning': 1, 'caution': 2, 'good': 3}
            feedback_messages.sort(
                key=lambda msg: min(
                    severity_order.get(evaluations[k]['status'], 3) 
                    for k in evaluations 
                    if evaluations[k].get('message') == msg
                )
            )
            
            # ========================================
            # Display feedback
            # ========================================
            
            # Pre-format strings
            text_data = [
                (f"R Elbow: {right_elbow_angle:.1f} deg", 30, evaluations['elbow_r']['color']),
                (f"L Elbow: {left_elbow_angle:.1f} deg", 60, evaluations['elbow_l']['color']),
                (f"R Knee: {right_knee_angle:.1f} deg", 90, evaluations['knee_r']['color']),
                (f"L Knee: {left_knee_angle:.1f} deg", 120, evaluations['knee_l']['color']),
                (f"Trunk Lean: {trunk_lean_angle:.1f} deg", 150, evaluations['trunk']['color']),
                (f"Step Length: {step_length:.1f} cm", 180, evaluations['step']['color']),
                (f"Base Width: {base_of_support:.1f} cm", 210, evaluations['base']['color']),
                (f"Shoulder Asym: {shoulder_elevation_percent:.1f}%", 240, evaluations['shoulder']['color']),
                (f"Leg Length: {avg_leg_length:.1f} cm", 270, (200, 200, 200))
            ]

            # Batch render text
            for text, y_pos, color in text_data:
                cv2.putText(image, text, (10, y_pos), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

            # Display top 3 feedback messages (RED for warnings)
            y_offset = 310
            for i, msg in enumerate(feedback_messages[:3]):
                cv2.putText(image, f"! {msg}", 
                           (10, y_offset + i*35), 
                           cv2.FONT_HERSHEY_SIMPLEX, 
                           0.5, (0, 0, 255), 2)

        # Show frame
        cv2.imshow("Mediapipe Feed", image)
        
        # Check if window was closed
        if cv2.getWindowProperty("Mediapipe Feed", cv2.WND_PROP_VISIBLE) < 1:
            cleanup_and_exit(cap)
        
        # Check for exit keys
        key = cv2.waitKey(10) & 0xFF
        if key == ord('q') or key == 27:
            cleanup_and_exit(cap)

# Fallback cleanup
cap.release()
cv2.destroyAllWindows()
print("âœ“ Processing complete")
